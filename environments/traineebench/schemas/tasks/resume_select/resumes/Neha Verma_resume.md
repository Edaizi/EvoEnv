## Neha Verma
Algorithm Researcher | (123) 456-7890 | neha.verma@email.com | linkedin.com/in/neha-verma

### Summary
Highly motivated and analytical Master's student in Electrical Engineering with a strong foundation in **Large Language Models (LLMs)**, machine learning algorithms, and signal processing. Passionate about developing innovative solutions for complex computational challenges. Currently pursuing advanced research in neural network architectures and **pre-training** methodologies. Seeking an Algorithm Researcher position to apply theoretical knowledge and practical skills in a cutting-edge R&D environment.

---

### Education
**Stanford University**
* **Master of Science (M.S.) in Electrical Engineering** (Expected May 2025)
    * *Research Focus:* Optimization of deep learning models for natural language understanding.

**Indian Institute of Technology (IIT) Delhi**
* **Bachelor of Technology (B.Tech.) in Electrical Engineering** (2019-2023)
    * *GPA:* 9.2/10.0

---

### Experience
**AI Innovations Lab - Research Assistant** | Stanford, CA
* **September 2023 – Present**
* Contributing to research on efficient **pre-training** techniques for domain-specific **Large Language Models**, reducing computational costs by an estimated 15%.
* Implemented and optimized novel attention mechanisms for transformer models, demonstrating improved performance on text generation tasks.
* Collaborated with a team of PhD students and faculty on a paper submitted to NeurIPS.

**Tech Solutions Inc. - Machine Learning Intern** | Bengaluru, India
* **May 2022 – August 2022 (3 Months)**
* Developed and evaluated machine learning models for anomaly detection in sensor data, achieving a 95% accuracy rate.
* Assisted in the data preprocessing and feature engineering for a recommendation system, improving click-through rates by 8%.
* Gained hands-on experience with Python, TensorFlow, and cloud-based ML platforms.

---

### Skills
* **Large Language Models (LLMs):** Transformer Architectures, **Pre-training**, Fine-tuning, Prompt Engineering, Causal Language Modeling, Seq2Seq Models.
* **Programming & Frameworks:** Python (PyTorch, TensorFlow, Keras, NumPy, Pandas, Scikit-learn), C++, MATLAB.
* **Machine Learning:** Deep Learning, Reinforcement Learning, Supervised/Unsupervised Learning, Algorithm Design, Optimization.
* **Tools & Platforms:** Git, Docker, AWS (EC2, S3), Google Cloud Platform (Vertex AI).
* **Electrical Engineering:** Signal Processing, Digital Systems, Control Systems.

---

### Projects
**Master's Thesis Project: Quantization-Aware Training for Edge LLMs**
* Designed and implemented a novel quantization-aware training pipeline to deploy compact **Large Language Models** on resource-constrained edge devices, achieving significant memory footprint reduction with minimal performance degradation.
* Conducted experiments on various LLM architectures (e.g., Llama-2 variants) using PyTorch.

---

### Achievements
* **Stanford Graduate Fellowship** (2023-2025)
* Awarded **Best Undergraduate Project** for "Real-time Object Detection using Embedded Systems" at IIT Delhi (2023).
* Published a co-authored paper, "Efficient Transformer Architectures for Low-Resource Languages," in a workshop at EMNLP (2023).